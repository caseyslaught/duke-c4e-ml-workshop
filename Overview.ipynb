{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b528b0-e065-4c07-9749-55afb1f24756",
   "metadata": {},
   "source": [
    "# üíªCoding for the Environment Machine Learning Workshop ü•≥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242c58c8-3dd0-4d16-9be3-d8cb0fb3a060",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "### 1. Introductions\n",
    "### 2. Setup\n",
    "### 3. Overview\n",
    "### 4. Clustering\n",
    "### 5. Supervised Classification\n",
    "### 6. Practice in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6fa98-ba1f-4f9c-9da6-a1ba8d4d7803",
   "metadata": {},
   "source": [
    "# 1. Introductions üëã"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d32707c-a367-4494-aab2-eac6af1a220c",
   "metadata": {},
   "source": [
    "# 2. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b121c62e-c00b-48a3-a5a0-a0ebe6df8641",
   "metadata": {},
   "source": [
    "### 1. Set up container: https://cmgr.oit.duke.edu/\n",
    "\n",
    "### 2. Clone workshop stuff:\n",
    "`git clone https://github.com/caseyslaught/duke-c4e-ml-workshop.git`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc3b2bd-d119-4b2d-90a7-29e1108de74a",
   "metadata": {},
   "source": [
    "# 3. Overview ü§î\n",
    "\n",
    "### Machine learning is a BIG field.\n",
    "- #### Land cover classification (what land cover class is this?)\n",
    "- #### Computer vision (what's in this photo?)\n",
    "- #### Audio processing (what animal is in this recording?)\n",
    "- #### Automated driving, natural language processing, automated robots, etc...\n",
    "\n",
    "<br/>\n",
    "\n",
    "![audio](images/spectrogram.jpeg)\n",
    "![computer vision](images/computer_vision.jpeg)\n",
    "![lulc](images/lulc.jpeg)\n",
    "\n",
    "### A brief history of ML\n",
    "- #### Machine learning has been around for a while!\n",
    "- #### First ML programs in the 1950's\n",
    "- #### Deep Blue beats Garry Kasparov in 1997\n",
    "- #### Deep learning coined in 2006\n",
    "- #### Recent explosion due to new computational ability (GPU's), data availablily, new techniques.\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "## We're gonna focus on classification in this workshop.\n",
    "\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## Unsupervised\n",
    "- #### No labeled data given\n",
    "- #### Goal is to find some structure in the data\n",
    "- #### ex. clustering\n",
    "\n",
    "![kmeans sat](images/kmeans_sat.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Supervised\n",
    "- #### We do have labeled data\n",
    "- #### We train a model using the labeled data\n",
    "- #### We then predict the output given a new set of inputs\n",
    "- #### ex. regression, random forest, neural networks\n",
    "\n",
    "\n",
    "## Clustering/classification problems\n",
    "- #### Given a point with some attributes, what group does it belong to?\n",
    "- #### ex. species (size, hair?, scales?, gills?) ‚Üí phylum\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a1a21b-d3e9-402f-8dde-11917169d54d",
   "metadata": {},
   "source": [
    "# 4. Clustering üßÆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8691e749-7e3f-4765-b045-b6bfbd296bf3",
   "metadata": {},
   "source": [
    "![](images/clusters_init.png)\n",
    "![](images/clusters_after.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b2e8d7-eca0-4d97-95b9-1dbafb1c18e2",
   "metadata": {},
   "source": [
    "## Clustering is all about grouping points using the data in out dataset\n",
    "\n",
    "### Why would we want to do this?\n",
    "- #### Understand the data better (maybe we don't know the groups in advance)\n",
    "- #### Explore potential classes for subsequent supervised analysis\n",
    "- #### Easily summarize data (kinda like compresion)\n",
    "\n",
    "### What are some potential problems?\n",
    "- #### Groups not neatly separated (lots of overlap, weird configurations)\n",
    "- Interpreting results takes domain knowledge\n",
    "\n",
    "![clusters](images/clustering.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f561b5-748e-4033-90b3-5889e10d8b41",
   "metadata": {},
   "source": [
    "## Questions so far?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82902891-ed64-4010-935a-c32bb4448139",
   "metadata": {},
   "source": [
    "## Clustering | K-Means\n",
    "\n",
    "### 1. Initialize\n",
    "- #### Define K cluster centers randomly (centroids)\n",
    "\n",
    "\n",
    "### 2. Iterate\n",
    "- #### For each point calculate closest cluster\n",
    "- #### Calculate mean of each cluster and make that mean the new cluster center  \n",
    "\n",
    "\n",
    "### 3. Terminate\n",
    "- #### If no points reassigned then finished\n",
    "\n",
    "<br />\n",
    "\n",
    "![kmeans](images/kmeans.png)\n",
    "![](images/kmeans_init.png)\n",
    "![](images/kmeans_after.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959d53c7-7b1f-4b7c-a5d7-b7e36bd3cbbf",
   "metadata": {},
   "source": [
    "## Clustering | K-Means (cont.)\n",
    "\n",
    "- ### Benefits\n",
    "    - #### Easy(ish) to understand\n",
    "- ### Downsides\n",
    "    - #### Very sensitive to initial cluster locations\n",
    "    - #### May terminate at local minimum (not global)\n",
    "    - #### May not know number of clusters in advance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed59052-a893-4a36-8727-e52911b2157b",
   "metadata": {},
   "source": [
    "## With K-means we might not end up at the optimal solution.\n",
    "### To get around this we can: \n",
    "#### 1. Calculate some metric of error for the K-means algorithms (ex. sum of distances to centroid)\n",
    "#### 2. Run the algorithm a bunch of times and save the results\n",
    "#### 3. Pick the result with the lowest error  \n",
    "\n",
    "\n",
    "![](images/kmeans_bad.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead948fc-9c5a-4da8-91d1-76217cb4689f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4092a4-50ce-41d7-8452-0a8879a2c876",
   "metadata": {},
   "source": [
    "# Clustering | Mixture of Gaussians\n",
    "\n",
    "### What if our clusters are not symetrical?\n",
    "### What if we have some overlap between cluseters?\n",
    "\n",
    "### Here's how Gaussian Mixture Models work:\n",
    "- #### Each group is represented by a Gaussian (normal) distribution\n",
    "- #### Each distribution has paramters: mean, covariance, and height\n",
    "- #### Goal is to find best set of parameters for the data\n",
    "- #### Use expectation-maximization (EM) algorithms to find optimal parameters\n",
    "\n",
    "<br/>\n",
    "\n",
    "![gaussian mixture](images/mixture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febea109-c1e4-4f9d-a207-2caa7dccfd98",
   "metadata": {},
   "source": [
    "### Let's revisit the previous example\n",
    "\n",
    "![](images/kmeans_init.png)\n",
    "![](images/mixture_after.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7837b09-edb1-4882-a8f7-d16f3d304602",
   "metadata": {},
   "source": [
    "## Clustering | Validation\n",
    "\n",
    "### How do we evaluate if our model is any good?\n",
    "#### With supervised learning we can withhold some of our labeled data to test our model.\n",
    "#### However, unsupervised learning doesn't have a source of truth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2db9bf-6158-420a-b48d-0d08174bdf05",
   "metadata": {},
   "source": [
    "#### Internal validation\n",
    "- #### Here we are asking, how cohesive (similar to each other) are clusters?\n",
    "- #### Also, how different are different clusters?\n",
    "- #### A good result (valid) will have high cohesion within clusters and high separation between clusters.\n",
    "\n",
    "<br/>\n",
    "\n",
    "![](images/internal_val.png)\n",
    "![](images/internal_val_score.png)\n",
    "#### Silhouette coefficient, Calisnki-Harabasz coefficient, Dunn index, Xie-Beni score, Hartigan index\n",
    "\n",
    "<br/>\n",
    "\n",
    "#### External validation \n",
    "- #### We can only due this if we have true labels for the clusters\n",
    "- #### Compare points from generated result to known clusters  \n",
    "![](images/external_val.png)\n",
    "### Jaccard Similarity, Mutual Information, Fowlkes-Mallows Index\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59896e1b-8576-4a0e-8599-c473422cc3b9",
   "metadata": {},
   "source": [
    "# 5. Supervised Learning\n",
    "### We have *labeled* data\n",
    "### We already know which classes to use\n",
    "\n",
    "## The general process of supervised classification is:\n",
    "### 1. Build a model using training data\n",
    "### 2. Evaluate that model using testing data\n",
    "### 3. Use that model on new, unseen data\n",
    "\n",
    "<br/>\n",
    "\n",
    "![](images/supervisedlearning.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83246df1-cc1f-49c6-b2ab-5f60e0c8785c",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "### 1. We start with a *root node*\n",
    "- #### No incoming links, but two or more outgoing links  \n",
    "\n",
    "### 2. We then go to an *internal node*, or condition\n",
    "- #### Conditions have one incoming link, and two or more outgoing links  \n",
    "- #### Conditions can use binary (yes/no), categorical (red, blue, green), or continuous **attributes** (3.141592...)\n",
    "\n",
    "### 3. We end with *leaf nodes* (our classes)\n",
    "- #### Leaf nodes have one incoming link and no outgoing links\n",
    "\n",
    "![decision tree](images/decisiontree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899e36cf-2959-4d53-a855-1a9828b72615",
   "metadata": {},
   "source": [
    "### **Root and internal nodes ü•ï make decisions on the data (tall or short)**\n",
    "### **Leaf nodes üçÉ provide the prediction (rhino or elephant)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97b92fb-7cc1-478e-9d57-f5848836a565",
   "metadata": {},
   "source": [
    "## Overview of decision trees\n",
    "\n",
    "- ### Handles different attribute types well (discrete, continuous)\n",
    "- ### Fitting model is fast, evaluation can be slow\n",
    "- ### Handles missing values well\n",
    "- ### Notorious for overfitting\n",
    "    - #### **Overfitting** is when our model does well on training data but not on testing or real-world scenarios\n",
    "    \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ebf4ae-c057-48a2-9ca8-030a5a8cae44",
   "metadata": {},
   "source": [
    "## Random Forest üå≤üå≤üå≤\n",
    "\n",
    "### Q: How can we avoid the *overfitting* problem with decision trees?\n",
    "### A: Let's create a bunch of decision trees and call it a forest!\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Q: But how do we train a bunch of very different decision tree with just one training set?\n",
    "### A: We can **resample** the training set to create a bunch of unique training sets.\n",
    "- #### We can either resample without replacement or *with* replacement (aka **bootstraping**)  \n",
    "\n",
    "<br/>\n",
    "\n",
    "### Q: How do we combine all of these trees into one decision ?\n",
    "### A: Take a majority vote. What is the most common decision across all trees?  \n",
    "\n",
    "<br/>\n",
    "\n",
    "![](images/randomforest.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe30207-20c9-421f-bf2c-8aeb74a4c102",
   "metadata": {},
   "source": [
    "## Here's how Random Forest works üå≥üå≥üå≥\n",
    "#### 1. Generate a training set using resampling with replacement (bootstrapping)\n",
    "#### 2. Set aside a fraction of training set for testing (aka *out-of-bag sample*)\n",
    "#### 3. Train many decision trees on the training set\n",
    "#### 4. Use majority vote to determine prediction\n",
    "#### 5. Use out-of-bag sample to perform cross-validation\n",
    "\n",
    "## üå≥üå≥üå≥ Notes about Random Forest üå¥üå¥üå¥\n",
    "\n",
    "- ### Combines the output of many decision trees achieve a single result (*ensemble method*)\n",
    "    - #### Reduces risk of overfitting\n",
    "- ### Can look up Gini index to get feature importance\n",
    "- ### Very solid algorithm!\n",
    "- ### More complex and time-consuming than simple decision tree\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55afc17a-c0d5-4e0e-82c4-1a45e3bcaa8b",
   "metadata": {},
   "source": [
    "## Support Vector Machine ‚ÜóÔ∏è‚ÜòÔ∏è\n",
    "- ### Classification technique that create a *hyperplane* between classes\n",
    "- ### In simplest form, only solves binary classification (dog or cat)\n",
    "- ### For multiple classes, we create a collection of many binary classiciations\n",
    "    - #### *one vs. one*\n",
    "    - #### *one vs. many*\n",
    " \n",
    "![](images/svm.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacfbf52-0834-41a9-89ff-fea6e19b74e4",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "### Let's look at how the brain works!\n",
    "### Neurons have many *dendrites* and an *axon*. Connections to other neurons are called *synapses*.\n",
    "### Activation function controls if a neuron *fires*\n",
    "\n",
    "![](images/neuron.jpg)\n",
    "\n",
    "\n",
    "### Artificial Neural Networks replicate this natural structure (kinda)\n",
    "- #### ANNs are at the hearth of *deep learning*\n",
    "- #### We start with an input layer (ex. Sentinel-2 band)\n",
    "- #### Each ANN has one or more hidden layers\n",
    "- #### We end with an output layer\n",
    "\n",
    "![](images/neuralnetwork.png)\n",
    "\n",
    "### Nodes, nodes, nodes\n",
    "- #### The node is the most basic unit (i.e. neuron)\n",
    "- #### A node receives one or more inputs (**X**)\n",
    "- #### It computes an output (**y**) and sends it to another node (or as the final output layer)\n",
    "- #### Each input (**x**) into a node has a *weight*\n",
    "- #### Each node has an activation function that computes the output\n",
    "\n",
    "\n",
    "![image.png](https://ujwlkarn.files.wordpress.com/2016/08/screen-shot-2016-08-09-at-3-42-21-am.png?w=1136&h=606)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86433d8e-88fb-4a76-b5cc-ea4f529351ab",
   "metadata": {},
   "source": [
    "## Weights, weights, weights\n",
    "### How do we determine what the weights should be?\n",
    "\n",
    "### Option 1: *feed-forward*\n",
    "- #### Remember that each neuron has an activation function and some weights\n",
    "- #### Multiply activations (*a*) by weights (*w*) and assign sum to a new neuron: new neuron = *a1\\*w1 + a2\\*w2 ...*\n",
    "- #### Continue this until output layer\n",
    "\n",
    "### Option 2: *backpropagation*\n",
    "- #### Start with the output layer\n",
    "- #### Going backwards, adjust weights to get desired output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba98fd35-0655-4878-8ff9-4105d94c5503",
   "metadata": {},
   "source": [
    "## This is a big, big field!\n",
    "\n",
    "![neuralnets](https://cdn-images-1.medium.com/max/2000/1*cuTSPlTq0a_327iTPJyD-Q.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd19d05-a272-48a5-a836-990a186739ea",
   "metadata": {},
   "source": [
    "## Final questions before diving into Python?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ef61f1-7d97-4d50-a7c4-ff1ce4093237",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Environmental Spatial Data Analysis by Nate Chaney, Duke University  \n",
    "https://github.com/chaneyn/ESDA_CEE690-02\n",
    "\n",
    "History of ML  \n",
    "https://www.forbes.com/sites/bernardmarr/2016/02/19/a-short-history-of-machine-learning-every-manager-should-read/?sh=66c6d5af15e7\n",
    "\n",
    "Awesome Deep Ecology by Patrick Gray, Duke University  \n",
    "https://github.com/patrickcgray/awesome-deep-ecology\n",
    "\n",
    "Unsupervised Validation  \n",
    "https://www.guavus.com/technical-blog/unsupervised-machine-learning-validation-techniques/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
